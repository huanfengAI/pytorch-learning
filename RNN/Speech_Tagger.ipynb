{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "epoch=0,loss=1.1114\n",
      "1\n",
      "1\n",
      "epoch=1,loss=1.1036\n",
      "1\n",
      "1\n",
      "epoch=2,loss=1.0961\n",
      "1\n",
      "tensor([1, 2, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "\n",
    "class Char_lstm(nn.Module):\n",
    "    def __init__(self,n_char,char_dim,char_hidden):\n",
    "        super(Char_lstm,self).__init__()\n",
    "        self.emb=nn.Embedding(n_char,char_dim)\n",
    "        self.lstm=nn.LSTM(char_dim,char_hidden)\n",
    "    def forward(self,x):\n",
    "        x=self.emb(x)\n",
    "        out,_=self.lstm(x)\n",
    "        return out[-1,:,:]\n",
    "\n",
    "def make_vector(x,w2i):\n",
    "    idx=[w2i[i.lower()] for i in x]\n",
    "    idx=torch.LongTensor(idx)\n",
    "    return idx\n",
    "class Lstm_tagger(nn.Module):\n",
    "    def __init__(self,n_word,n_char,char_dim,word_dim,char_hidden,word_hidden,n_tag):\n",
    "        super(Lstm_tagger,self).__init__()\n",
    "        self.char_lstm=Char_lstm(n_char,char_dim,char_hidden)\n",
    "        self.word_embed=nn.Embedding(n_word,word_dim)\n",
    "        self.word_lstm=nn.LSTM(word_dim+char_hidden,word_hidden)\n",
    "        self.classify=nn.Linear(word_hidden,n_tag)\n",
    "    def forward(self,word_list,words):\n",
    "        #word_list和words一致的\n",
    "        char=[]\n",
    "        for word in words:#words是每一个单词\n",
    "            char_list=make_vector(word,char_to_idx)\n",
    "            char_list=char_list.unsqueeze(1)#batch=1\n",
    "            char_infor=self.char_lstm(torch.tensor(char_list))\n",
    "            char.append(char_infor)\n",
    "        \n",
    "        char=torch.stack(char,dim=0)\n",
    "        x=self.word_embed(word_list).unsqueeze(1)\n",
    "        x=torch.cat((x,char),dim=2)\n",
    "        x,_=self.word_lstm(x)\n",
    "        s,b,h=x.shape\n",
    "        print(b)\n",
    "        x=x.view(-1,h)\n",
    "        out=self.classify(x)\n",
    "        return out\n",
    "\n",
    "training_data = [(\"The monkey ate the banana\".split(),\n",
    "                  [\"DET\", \"NN\", \"V\", \"DET\",\"NN\"]),\n",
    "                 (\"The dog ate the bones\".split(), \n",
    "                  [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"])]\n",
    "w2i=defaultdict(lambda :len(w2i))\n",
    "t2i=defaultdict(lambda :len(t2i))\n",
    "for context,tag in training_data:\n",
    "    for words in context:\n",
    "        index=w2i[words.lower()]\n",
    "    for label in tag:\n",
    "        index=t2i[label.lower()]\n",
    "        \n",
    "        \n",
    "z = 'abcdefghijklmnopqrstuvwxyz'\n",
    "char_to_idx={}\n",
    "for i in range(len(z)):\n",
    "    char_to_idx[z[i]]=i\n",
    "#参数分别为：\n",
    "#单词字典的长度\n",
    "#字符字典的长度\n",
    "#字符的emb_size\n",
    "#单词的emb_size\n",
    "#字符的lstm的hidden\n",
    "#单词的lstm的hidden\n",
    "#分类的类别\n",
    "net=Lstm_tagger(len(w2i),len(char_to_idx),10, 100, 50, 128, len(t2i))\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(net.parameters(),lr=0.01)\n",
    "for epoch in range(3):\n",
    "    train_loss=0.0\n",
    "    for words,tag in training_data:#word,tag是文本和文本标签\n",
    "        word_list=make_vector(words,w2i)\n",
    "        tag=make_vector(tag,t2i)\n",
    "        out=net(word_list,words)\n",
    "        loss=criterion(out,tag)\n",
    "        train_loss+=loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch=%r,loss=%.4f\"%(epoch,train_loss/len(training_data)))\n",
    "\n",
    "\n",
    "net = net.eval()\n",
    "test_sent = 'The dog ate the banana'\n",
    "test = make_vector(test_sent.split(), w2i)\n",
    "\n",
    "out = net(test, test_sent.split())\n",
    "print(out.max(1)[1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
